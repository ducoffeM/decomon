{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deel.lip.activations import GroupSort2\n",
    "from deel.lip.layers import (\n",
    "    FrobeniusDense,\n",
    "    ScaledL2NormPooling2D,\n",
    "    SpectralConv2D,\n",
    "    SpectralDense,\n",
    ")\n",
    "from deel.lip.losses import MulticlassHKR, MulticlassKR\n",
    "from deel.lip.model import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Flatten, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    SpectralDense(100, kernel_initializer=\"orthogonal\"),\n",
    "    GroupSort2(),\n",
    "    SpectralDense(100, kernel_initializer=\"orthogonal\"),\n",
    "    GroupSort2(),\n",
    "    SpectralDense(32, kernel_initializer=\"orthogonal\"),\n",
    "    GroupSort2(),\n",
    "    FrobeniusDense(10, activation=None, use_bias=False, kernel_initializer=\"orthogonal\"),\n",
    "]\n",
    "\n",
    "model = Sequential(\n",
    "    layers,\n",
    "    k_coef_lip=1.0,\n",
    "    name=\"hkr_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b084b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    # decreasing alpha and increasing min_margin improve robustness (at the cost of accuracy)\n",
    "    # note also in the case of lipschitz networks, more robustness require more parameters.\n",
    "    loss=MulticlassHKR(alpha=50, min_margin=0.05),\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=[\"accuracy\", MulticlassKR()],\n",
    ")\n",
    "\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# standardize and reshape the data\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_test = (x_test - mean) / std\n",
    "# one hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once training is finished you can convert\n",
    "# SpectralDense layers into Dense layers and SpectralConv2D into Conv2D\n",
    "# which optimize performance for inference\n",
    "vanilla_model = model.vanilla_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b84681",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decomon import get_adv_box\n",
    "from decomon.models import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fb582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d21648",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomon_model = clone(vanilla_model, method=\"forward-ibp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ba0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomon_model.predict(np.zeros((1, 2, 784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b594857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf86641",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        # Lipschitz layers preserve the API of their superclass ( here Conv2D )\n",
    "        # an optional param is available: k_coef_lip which control the lipschitz\n",
    "        # constant of the layer\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        # usual pooling layer are implemented (avg, max...), but new layers are also available\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        # our layers are fully interoperable with existing keras layers\n",
    "        Flatten(),\n",
    "        SpectralDense(\n",
    "            32,\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        FrobeniusDense(10, activation=None, use_bias=False, kernel_initializer=\"orthogonal\"),\n",
    "    ],\n",
    "    # similary model has a parameter to set the lipschitz constant\n",
    "    # to set automatically the constant of each layer\n",
    "    k_coef_lip=1.0,\n",
    "    name=\"hkr_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba21ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential (resp Model) from deel.model has the same properties as any lipschitz model.\n",
    "# It act only as a container, with features specific to lipschitz\n",
    "# functions (condensation, vanilla_exportation...) but The layers are fully compatible\n",
    "# with the tf.keras.model.Sequential/Model\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28, 1)),\n",
    "        # Lipschitz layers preserve the API of their superclass ( here Conv2D )\n",
    "        # an optional param is available: k_coef_lip which control the lipschitz\n",
    "        # constant of the layer\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        # usual pooling layer are implemented (avg, max...), but new layers are also available\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        SpectralConv2D(\n",
    "            filters=16,\n",
    "            kernel_size=(3, 3),\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        # our layers are fully interoperable with existing keras layers\n",
    "        Flatten(),\n",
    "        SpectralDense(\n",
    "            32,\n",
    "            activation=GroupSort(2),\n",
    "            use_bias=True,\n",
    "            kernel_initializer=\"orthogonal\",\n",
    "        ),\n",
    "        FrobeniusDense(10, activation=None, use_bias=False, kernel_initializer=\"orthogonal\"),\n",
    "    ],\n",
    "    # similary model has a parameter to set the lipschitz constant\n",
    "    # to set automatically the constant of each layer\n",
    "    k_coef_lip=1.0,\n",
    "    name=\"hkr_model\",\n",
    ")\n",
    "\n",
    "# HKR (Hinge-Krantorovich-Rubinstein) optimize robustness along with accuracy\n",
    "model.compile(\n",
    "    # decreasing alpha and increasing min_margin improve robustness (at the cost of accuracy)\n",
    "    # note also in the case of lipschitz networks, more robustness require more parameters.\n",
    "    loss=MulticlassHKR(alpha=50, min_margin=0.05),\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=[\"accuracy\", MulticlassKR()],\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# standardize and reshape the data\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "mean = x_train.mean()\n",
    "std = x_train.std()\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_test = (x_test - mean) / std\n",
    "# one hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# fit the model\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test, y_test),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# once training is finished you can convert\n",
    "# SpectralDense layers into Dense layers and SpectralConv2D into Conv2D\n",
    "# which optimize performance for inference\n",
    "vanilla_model = model.vanilla_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
